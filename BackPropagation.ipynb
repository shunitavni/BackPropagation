{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunitavni/BackPropagation/blob/master/BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZea8MyRkvS4",
        "colab_type": "text"
      },
      "source": [
        "implement a full backprop algorithm using only *numpy*.\n",
        "\n",
        "- We assume sigmoid activation across all layers.\n",
        "- We assume a single value in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdK1awC1ksco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK-mAqGCk3Wp",
        "colab_type": "text"
      },
      "source": [
        "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq8Fop28k2Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNN:\n",
        "  def __init__(self, learning_rate, layer_sizes):\n",
        "    '''\n",
        "    learning_rate - the learning to use in backward\n",
        "    layer_sizes - a list of numbers, each number represents the number of neurons\n",
        "                  to have in every layer. Therefore, the length of the list \n",
        "                  represents the number of layers this network has.\n",
        "    '''\n",
        "    self.learning_rate = learning_rate\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.model_params = {}\n",
        "    self.memory = {}\n",
        "    self.grads = {}\n",
        "    \n",
        "    # Initializing weights\n",
        "    for layer_index in range(len(layer_sizes) - 1):\n",
        "      W_input = layer_sizes[layer_index + 1]\n",
        "      W_output = layer_sizes[layer_index]\n",
        "      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
        "      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
        "      \n",
        "      \n",
        "  def forward_single_instance(self, x):    \n",
        "    a_i_1 = x\n",
        "    self.memory['a_0'] = x\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "      z_i = np.dot(W_i, a_i_1) + b_i\n",
        "      a_i = 1/(1+np.exp(-z_i))\n",
        "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
        "      a_i_1 = a_i\n",
        "    return a_i_1\n",
        "  \n",
        "  \n",
        "  def log_loss(y_hat, y):\n",
        "    '''\n",
        "    Logistic loss, assuming a single value in y_hat and y.\n",
        "    '''\n",
        "    m = y_hat[0]\n",
        "    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
        "    return cost\n",
        "  \n",
        "  def backward_single_instance(self, y):\n",
        "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
        "    dz = a_output - y\n",
        "     \n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
        "      print(layer_index)\n",
        "      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
        "      self.grads['db_' + str(layer_index)] = dz.reshape((-1,));\n",
        "      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
        "\n",
        "  \n",
        "  # TODO: update weights with grads\n",
        "  def update(self):\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "        #print(layer_index);\n",
        "        self.model_params['W_' + str(layer_index + 1)] -= self.learning_rate * self.grads['dW_' + str(layer_index + 1)]\n",
        "        self.model_params['b_' + str(layer_index + 1)] -= self.learning_rate * self.grads['db_' + str(layer_index + 1)]\n",
        "  \n",
        "  # TODO: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n",
        "  def forward_batch(self, X):\n",
        "    a_i_1 = X\n",
        "    self.memory['a_0'] = X\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "      z_i = np.dot(W_i, a_i_1) + b_i.reshape(-1,1);\n",
        "      a_i = 1/(1+np.exp(-z_i))\n",
        "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
        "      a_i_1 = a_i\n",
        "    return a_i_1     \n",
        "  \n",
        "  # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n",
        "  def backward_batch(self, y):\n",
        "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
        "    dz = a_output - y\n",
        "     \n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
        "      #print(layer_index)\n",
        "      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
        "      self.grads['db_' + str(layer_index)] = np.mean(dz)#np.sum(dz);\n",
        "      dW = np.dot(dz,a_l_1.T) / 8;\n",
        "      #dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      dz = (a_l_1 * (1 - a_l_1)) * np.dot(W_l.T, dz)\n",
        "      \n",
        "  \n",
        "  # TODO: implement log_loss_batch, for a batch of instances\n",
        "  def log_loss_batch(self, y_hat, y):\n",
        "      return np.mean((-y*np.log(y_hat)) - ((1-y)*np.log(1-y_hat)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbbIIDX-k6Zx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1b3a4f54-3463-430d-956c-8b0d639214bf"
      },
      "source": [
        "# Create a simple neural network to test single propogation\n",
        "nn = MyNN(0.01, [3, 2, 1])\n",
        "# Print the model parameters\n",
        "nn.model_params\n",
        "# Create mock data\n",
        "x = np.random.randn(3)\n",
        "y = np.random.randn(1)\n",
        "# Forward\n",
        "y_hat = nn.forward_single_instance(x)\n",
        "# Backward\n",
        "nn.backward_single_instance(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlWYeMQllNWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the train function for both batch and single implementations\n",
        "def train(X, y, epochs, batch_size):\n",
        "  for e in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    #shuffle\n",
        "    X_shuffle,y_shuffle= shuffle(X.T,y.T)\n",
        "    X_shuffle = X_shuffle.T\n",
        "    y_shuffle = y_shuffle.T\n",
        "    #divide to batches\n",
        "    numBatches = X_shuffle.shape[1]/batch_size\n",
        "    batches = zip(np.split(X_shuffle,numBatches,axis=1),np.split(y_shuffle,numBatches,axis=1))\n",
        "    for X_b, y_b in batches:\n",
        "      y_hat = nn.forward_batch(X_b)\n",
        "      epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
        "      nn.backward_batch(y_b)\n",
        "      nn.update()\n",
        "    print(f'Epoch {e}, loss={epoch_loss/numBatches}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNTQnC5bmALX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cd3f4784-e410-460c-b79e-77795c0c8ffc"
      },
      "source": [
        "#Make sure the following network trains properly\n",
        "\n",
        "# Create a dense feed forward network\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "# Mock features\n",
        "X = np.random.randn(6, 100)\n",
        "# Mock Labels\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss=0.6101103228733691\n",
            "Epoch 2, loss=0.6058136101347806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XlxrlwtmPfo",
        "colab_type": "text"
      },
      "source": [
        "#TODO: train on an external dataset\n",
        "\n",
        "Train on the Bike Sharing dataset, using the same split as in *DL Notebook 4 - logistic regression*.\n",
        "Use the following features from the data:\n",
        "\n",
        "* temp\n",
        "* atemp\n",
        "* hum\n",
        "* windspeed\n",
        "* weekday\n",
        "\n",
        "The response variable is, like in Notebook 4, raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"]).\n",
        "\n",
        "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
        "\n",
        "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
        "\n",
        "Then, plot loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbyuFskYmN8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c5bab30-1fa4-434f-a128-75e2c5d99863"
      },
      "source": [
        "# Imports\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Mount the drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjHAU_Y1nPpX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "dbb41c3d-206d-4fb9-e95d-b4f9f959c365"
      },
      "source": [
        "# Load the data\n",
        "raw = pd.read_csv('/content/drive/My Drive/DL Course/Assignment 1/day.csv')\n",
        "# Show the head of the data\n",
        "raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.344167</td>\n",
              "      <td>0.363625</td>\n",
              "      <td>0.805833</td>\n",
              "      <td>0.160446</td>\n",
              "      <td>331</td>\n",
              "      <td>654</td>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.363478</td>\n",
              "      <td>0.353739</td>\n",
              "      <td>0.696087</td>\n",
              "      <td>0.248539</td>\n",
              "      <td>131</td>\n",
              "      <td>670</td>\n",
              "      <td>801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-03</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.196364</td>\n",
              "      <td>0.189405</td>\n",
              "      <td>0.437273</td>\n",
              "      <td>0.248309</td>\n",
              "      <td>120</td>\n",
              "      <td>1229</td>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-04</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.212122</td>\n",
              "      <td>0.590435</td>\n",
              "      <td>0.160296</td>\n",
              "      <td>108</td>\n",
              "      <td>1454</td>\n",
              "      <td>1562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-05</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.226957</td>\n",
              "      <td>0.229270</td>\n",
              "      <td>0.436957</td>\n",
              "      <td>0.186900</td>\n",
              "      <td>82</td>\n",
              "      <td>1518</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   instant      dteday  season  yr  ...  windspeed  casual  registered   cnt\n",
              "0        1  2011-01-01       1   0  ...   0.160446     331         654   985\n",
              "1        2  2011-01-02       1   0  ...   0.248539     131         670   801\n",
              "2        3  2011-01-03       1   0  ...   0.248309     120        1229  1349\n",
              "3        4  2011-01-04       1   0  ...   0.160296     108        1454  1562\n",
              "4        5  2011-01-05       1   0  ...   0.186900      82        1518  1600\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvJ0QO7hnVv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dataset\n",
        "\n",
        "# Feature space\n",
        "X = np.array([raw[\"temp\"].values,raw[\"atemp\"].values,raw[\"hum\"].values,raw[\"windspeed\"].values,raw[\"weekday\"].values]).T\n",
        "# Labels space\n",
        "result = np.array((raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"])).values);\n",
        "y = np.where(result == True,1,0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLs06Z6Tn3LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split to training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDmAF1gnn8rS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "464018b8-a324-4749-af97-78a1f13e96d5"
      },
      "source": [
        "# Create a dense feed forward network\n",
        "nn = MyNN(0.001,  [5, 40, 30, 10, 7, 5, 3, 1])\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "\n",
        "train(X_train.T, y_train.reshape(1,-1), epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss=0.6836594854985163\n",
            "Epoch 2, loss=0.683661403355386\n",
            "Epoch 3, loss=0.6836671242198937\n",
            "Epoch 4, loss=0.6836552038813869\n",
            "Epoch 5, loss=0.683660182728087\n",
            "Epoch 6, loss=0.683658011728688\n",
            "Epoch 7, loss=0.6836482206158816\n",
            "Epoch 8, loss=0.6836511684592725\n",
            "Epoch 9, loss=0.6836507747184973\n",
            "Epoch 10, loss=0.6836419753733919\n",
            "Epoch 11, loss=0.6836503112584242\n",
            "Epoch 12, loss=0.6836356603923054\n",
            "Epoch 13, loss=0.6836493023083159\n",
            "Epoch 14, loss=0.6836563500779179\n",
            "Epoch 15, loss=0.68365934113999\n",
            "Epoch 16, loss=0.683652780368265\n",
            "Epoch 17, loss=0.6836572068791072\n",
            "Epoch 18, loss=0.6836397878528199\n",
            "Epoch 19, loss=0.683659966112989\n",
            "Epoch 20, loss=0.6836316815967406\n",
            "Epoch 21, loss=0.6836346967827212\n",
            "Epoch 22, loss=0.6836316859129873\n",
            "Epoch 23, loss=0.6836268454856907\n",
            "Epoch 24, loss=0.6836510481523681\n",
            "Epoch 25, loss=0.6836504782163374\n",
            "Epoch 26, loss=0.6836448711694286\n",
            "Epoch 27, loss=0.6836433454980475\n",
            "Epoch 28, loss=0.68363612355509\n",
            "Epoch 29, loss=0.6836418693466276\n",
            "Epoch 30, loss=0.6836360863393415\n",
            "Epoch 31, loss=0.6836504327212802\n",
            "Epoch 32, loss=0.6836357734168902\n",
            "Epoch 33, loss=0.6836644847206147\n",
            "Epoch 34, loss=0.6836330263639159\n",
            "Epoch 35, loss=0.6836615333544553\n",
            "Epoch 36, loss=0.683630460025291\n",
            "Epoch 37, loss=0.6836359010146762\n",
            "Epoch 38, loss=0.6836422409148541\n",
            "Epoch 39, loss=0.6836325009766899\n",
            "Epoch 40, loss=0.6836291876560605\n",
            "Epoch 41, loss=0.6836410967522272\n",
            "Epoch 42, loss=0.6836381372928629\n",
            "Epoch 43, loss=0.6836285334871757\n",
            "Epoch 44, loss=0.6836328417426586\n",
            "Epoch 45, loss=0.6836294202406398\n",
            "Epoch 46, loss=0.6836428153942876\n",
            "Epoch 47, loss=0.6835993200245601\n",
            "Epoch 48, loss=0.6836326154528489\n",
            "Epoch 49, loss=0.6836520795952765\n",
            "Epoch 50, loss=0.6836330744088807\n",
            "Epoch 51, loss=0.6836270462402836\n",
            "Epoch 52, loss=0.6836413448740504\n",
            "Epoch 53, loss=0.6836167922616386\n",
            "Epoch 54, loss=0.6836381142371497\n",
            "Epoch 55, loss=0.6836353658006521\n",
            "Epoch 56, loss=0.6836470038409312\n",
            "Epoch 57, loss=0.6836260866573335\n",
            "Epoch 58, loss=0.683647162978162\n",
            "Epoch 59, loss=0.6836501387984412\n",
            "Epoch 60, loss=0.6836409721478814\n",
            "Epoch 61, loss=0.6836628268138476\n",
            "Epoch 62, loss=0.6836185871511403\n",
            "Epoch 63, loss=0.6836338915349442\n",
            "Epoch 64, loss=0.6836324838716861\n",
            "Epoch 65, loss=0.6836444339479876\n",
            "Epoch 66, loss=0.6836440982297762\n",
            "Epoch 67, loss=0.6836218059142697\n",
            "Epoch 68, loss=0.6836423163395013\n",
            "Epoch 69, loss=0.6836251646843783\n",
            "Epoch 70, loss=0.683618749292894\n",
            "Epoch 71, loss=0.6836404859803813\n",
            "Epoch 72, loss=0.6836520551340503\n",
            "Epoch 73, loss=0.6836407299188182\n",
            "Epoch 74, loss=0.6836348075581651\n",
            "Epoch 75, loss=0.6836694947151154\n",
            "Epoch 76, loss=0.6836304353850363\n",
            "Epoch 77, loss=0.6836250888079343\n",
            "Epoch 78, loss=0.6836477426786756\n",
            "Epoch 79, loss=0.6836590182951132\n",
            "Epoch 80, loss=0.6836322839758822\n",
            "Epoch 81, loss=0.6836353324942663\n",
            "Epoch 82, loss=0.6836521996667895\n",
            "Epoch 83, loss=0.6836100853132909\n",
            "Epoch 84, loss=0.6836271480994336\n",
            "Epoch 85, loss=0.6836370862335635\n",
            "Epoch 86, loss=0.6836513987211592\n",
            "Epoch 87, loss=0.6836066962146758\n",
            "Epoch 88, loss=0.6836303528702956\n",
            "Epoch 89, loss=0.683643893543213\n",
            "Epoch 90, loss=0.6836737774992969\n",
            "Epoch 91, loss=0.6836115221747957\n",
            "Epoch 92, loss=0.6836434503745521\n",
            "Epoch 93, loss=0.6836397793610207\n",
            "Epoch 94, loss=0.6836262696332145\n",
            "Epoch 95, loss=0.6836392896577023\n",
            "Epoch 96, loss=0.6836230256636675\n",
            "Epoch 97, loss=0.6836410844712375\n",
            "Epoch 98, loss=0.6836421509016365\n",
            "Epoch 99, loss=0.6836670316916986\n",
            "Epoch 100, loss=0.6836265112902444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj1HkBY8phZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the network to predict the values for X_test\n",
        "pred = nn.forward_batch(X_test.T);\n",
        "# This architecture does not seem to be a fitting hypothesis for the data set.\n",
        "# using a trivial treshold function (1 if y_pred > 0.5) returns false for all the test set.\n",
        "y_pred = np.where(pred > 0.5,1,0).reshape(-1,1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x65A5zgbvQ5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = y_test.reshape(-1,1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcxJXvLWv9iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0up_QmnuNcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "de0ea08c-a871-4794-a641-6b37e664898a"
      },
      "source": [
        "target_names = ['class 0', 'class 1'];\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "# Plot non-normalized confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred);\n",
        "df_cm = pd.DataFrame(cm, range(len(cm)),\n",
        "                  range(len(cm)))\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.set(font_scale=1.4)#for label size\n",
        "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00       106\n",
            "     class 1       0.52      1.00      0.68       114\n",
            "\n",
            "    accuracy                           0.52       220\n",
            "   macro avg       0.26      0.50      0.34       220\n",
            "weighted avg       0.27      0.52      0.35       220\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAE2CAYAAAADPAuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u6T1ZkHuSRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}